# SemiBin2 v2.2.1

Deep learning-based metagenomic binning tool with pre-trained models for various environments.

## Official Documentation
- GitHub: https://github.com/BigDataBiology/SemiBin
- Version: 2.2.1

## Installation

**Conda (recommended):**
```bash
conda create -n semibin2
conda activate semibin2
conda install -c conda-forge -c bioconda semibin
```

**With GPU support (faster):**
```bash
pixi install semibin pytorch-gpu cuda
```

**Supported Python versions:** 3.8-3.13

## Key Command-Line Flags

### Single-Sample Mode

| Flag | Description |
|------|-------------|
| `-i, --input-fasta` | Input contig assembly file (FASTA) |
| `-b, --input-bam` | Sorted BAM file from read mapping |
| `-o, --output` | Output directory |
| `--environment` | Pre-trained model environment |
| `--sequencing-type` | Sequencing type (default: short_read, or long_read) |
| `-p, --processes` | Number of CPU processes |
| `--minfasta-kbs` | Minimum contig length in kb (default: 1) |

### Multi-Sample Mode

| Flag | Description |
|------|-------------|
| `-i` | Combined contig file with `sample:contig` format |
| `-b` | Multiple BAM files (wildcards supported) |
| `-s, --separator` | Separator between sample and contig names (default: `:`) |
| `--self-supervised` | Use self-supervised learning (no pre-trained model) |

### Training Mode (Advanced)

| Flag | Description |
|------|-------------|
| `--mode` | Training mode (several, single, combined) |
| `--train-from-many` | Train from multiple samples |

## Pre-Trained Environment Models

SemiBin2 includes environment-specific models:
- `human_gut` - Human gut microbiome
- `dog_gut` - Dog gut microbiome
- `ocean` - Marine environments
- `soil` - Soil microbiomes
- `cat_gut` - Cat gut microbiome
- `human_oral` - Human oral microbiome
- `mouse_gut` - Mouse gut microbiome
- `pig_gut` - Pig gut microbiome
- `built_environment` - Built environments
- `wastewater` - Wastewater treatment
- `chicken_caecum` - Chicken caecum
- `global` - Universal model (default fallback)

## Common Usage Examples

### Single-sample with pre-trained model
```bash
SemiBin2 single_easy_bin \
  -i contigs.fasta \
  -b reads.sorted.bam \
  --environment human_gut \
  -o output_semibin \
  -p 16
```

### Multi-sample binning
```bash
# Prepare concatenated contigs with sample prefixes
# Format: sample1:contig001, sample1:contig002, sample2:contig001, etc.

SemiBin2 multi_easy_bin \
  -i concatenated_contigs.fasta \
  -b sample*.sorted.bam \
  -o output_semibin \
  -p 16
```

### Long-read assembly binning
```bash
SemiBin2 single_easy_bin \
  -i contigs.fasta \
  -b reads.sorted.bam \
  --environment global \
  --sequencing-type long_read \
  -o output_semibin \
  -p 16
```

### Self-supervised mode (no pre-trained model)
```bash
SemiBin2 multi_easy_bin \
  -i concatenated_contigs.fasta \
  -b *.sorted.bam \
  --self-supervised \
  -o output_semibin \
  -p 16
```

## Input Requirements

**Contig file:**
- FASTA format
- Minimum length: 1000 bp (default, configurable with `--minfasta-kbs`)
- For multi-sample: concatenated with sample identifier prefix

**BAM files:**
- Sorted alignment files
- Generated by mapping reads to contigs
- Multiple BAM files for multi-sample mode

Example mapping workflow:
```bash
# Map reads to contigs
minimap2 -ax sr -t 16 contigs.fasta reads_1.fq.gz reads_2.fq.gz | \
  samtools sort -@ 8 -o reads.sorted.bam -
samtools index reads.sorted.bam
```

## Output Format

**Output directory structure:**
```
output_semibin/
├── output_bins/              # Final bins
│   ├── bin.0.fa
│   ├── bin.1.fa
│   └── ...
├── output_recluster_bins/    # Reclustered bins (if applicable)
│   ├── recluster.0.fa
│   └── ...
├── data.csv                  # Contig data
├── model.h5                  # Trained model
└── bin_summary.tsv           # Bin statistics
```

**Bin summary includes:**
- Bin ID
- Number of contigs
- Total length
- N50
- L50

## Performance Tips

1. **Use pre-trained models**: Faster and often more accurate than self-supervised
2. **GPU acceleration**: Significant speedup with CUDA-enabled PyTorch
3. **Multi-sample preferred**: Better binning quality with 3+ samples
4. **Memory**: ~8-16 GB RAM for typical datasets
5. **Minimum contig length**: Default 1 kb is reasonable; increase for large datasets
6. **CPU processes**: Scale with available cores (`-p`)

## Advanced Features

### Training Custom Models

For unique environments not covered by pre-trained models:
```bash
SemiBin2 train \
  --data data_dir/ \
  --mode combined \
  -o custom_model/
```

**Warning:** Training is resource-intensive (disk space, time, memory)

### Concatenating Samples

For multi-sample binning, concatenate contigs with sample identifiers:
```bash
# Example: Add sample prefix to each contig
for sample in sample1 sample2 sample3; do
  sed "s/^>/>${sample}:/" ${sample}/contigs.fasta
done > concatenated_contigs.fasta
```

## Model Selection Strategy

1. **Known environment**: Use matching pre-trained model (e.g., `human_gut`)
2. **Unknown environment**: Use `global` model (trained on diverse datasets)
3. **No good match**: Use `--self-supervised` mode
4. **Multiple environments**: Consider training custom model

## Integration with QC Tools

```bash
# Run SemiBin2
SemiBin2 single_easy_bin -i contigs.fasta -b reads.bam \
  --environment human_gut -o semibin_output -p 16

# Assess bin quality
checkm2 predict --input semibin_output/output_bins/ \
  --output-directory qc_results/ -t 16
```

## Common Issues

- **Low quality bins**: Try different environment model or self-supervised mode
- **Too few bins**: Check coverage depth, consider increasing `--minfasta-kbs`
- **GPU errors**: Ensure CUDA and PyTorch versions are compatible
- **Memory issues**: Reduce batch size or process fewer samples

## Comparison with Other Binners

**Advantages:**
- Deep learning approach with environment-specific training
- Often higher precision than traditional methods
- Fast with GPU acceleration

**Considerations:**
- Requires adequate training data for custom models
- May be less effective on novel/underrepresented taxa
- Works best with sufficient coverage (>5-10x)

## Citation

Pan S, Zhu C, Zhao XM, Coelho LP. (2022)
A deep siamese neural network improves metagenome-assembled genomes in microbiome datasets across different environments.
*Nature Communications* 13:2326. https://doi.org/10.1038/s41467-022-29843-y
